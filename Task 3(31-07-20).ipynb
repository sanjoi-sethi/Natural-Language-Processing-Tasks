{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in c:\\users\\sanjoi\\anaconda3\\lib\\site-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Chinese Segmentation using jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg=jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\", cut_all=True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Basic Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Beauty', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('property', 'NN'), ('of', 'IN'), ('certain', 'JJ'), ('things', 'NNS'), ('.', '.')]\n",
      "[('Something', 'VBG'), ('is', 'VBZ'), ('beautiful', 'JJ'), ('if', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('nice', 'JJ'), ('to', 'TO'), ('look', 'VB'), ('at', 'IN'), ('it', 'PRP'), (',', ','), ('hear', 'VB'), ('it', 'PRP'), (',', ','), ('feel', 'VB'), ('it', 'PRP'), (',', ','), ('taste', 'NN'), ('it', 'PRP'), (',', ','), ('smell', 'VBZ'), ('it', 'PRP'), ('or', 'CC'), ('think', 'VB'), ('about', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('is', 'VBZ'), ('also', 'RB'), ('the', 'DT'), ('name', 'NN'), ('of', 'IN'), ('a', 'DT'), ('feeling', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('describe', 'VB'), ('.', '.')]\n",
      "[('The', 'DT'), ('nature', 'NN'), ('of', 'IN'), ('this', 'DT'), ('feeling', 'NN'), ('varies', 'VBZ'), ('from', 'IN'), ('person', 'NN'), ('to', 'TO'), ('person', 'NN'), ('and', 'CC'), ('culture', 'NN'), ('to', 'TO'), ('culture', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('known', 'VBN'), ('if', 'IN'), ('only', 'RB'), ('humans', 'NNS'), ('can', 'MD'), ('feel', 'VB'), ('it', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "texts=\"\"\"Beauty is a property of certain things. \n",
    "Something is beautiful if it is nice to look at it, hear it, feel it, taste it, smell it or think about it.\n",
    "It is also the name of a feeling that is hard to describe. \n",
    "The nature of this feeling varies from person to person and culture to culture.\n",
    "It is not known if only humans can feel it.\"\"\"\n",
    "sentences=nltk.sent_tokenize(texts)\n",
    "for sentence in sentences:\n",
    "    words=nltk.word_tokenize(sentence)\n",
    "    tagged=nltk.pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Party', 'was', 'soooo', 'fun', ':D', '#superfun']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "text=\"The Party was soooo fun :D #superfun\"\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Scraping Data From Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url=\"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response=request.urlopen(url)\n",
    "raw=response.read().decode('utf8')\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Crime and Punishme\n"
     ]
    }
   ],
   "source": [
    "print(raw[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: ﻿The Project Gutenberg EBook of Crime and Punishment...>\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'only', 'It', 'feel', 'known', 'if', 'can', '.', 'humans', 'is', 'it', 'not'}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML=> ASCII=> Token=> Text=> Vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
